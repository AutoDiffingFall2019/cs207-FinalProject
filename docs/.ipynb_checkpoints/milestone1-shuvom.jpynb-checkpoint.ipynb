{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Notes\n",
    "The purpose of this section is to give a sketch of the mathematical background of automatic differentiation to motivate our implementation of the forward mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "We intend to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Classes and Data Structures\n",
    "Recall that automatic differentation is a method for computing the Jacobian of a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ by accumulating and combining partial derivates of elementary functions.  In the _forward mode_ of automatic differentiation, we observe that $g(\\alpha + \\beta \\epsilon) = g(\\alpha) + g'(\\alpha)\\beta\\epsilon$ where $\\alpha + \\beta\\epsilon$ is the dual representation of the real $\\alpha$ (with nilpotent $\\epsilon$, $\\epsilon^2 > 0$).  This observation, along with the chain rule, allows us to compute the Jacobian of $f$ as:\n",
    "$$\\frac{df_j(x)}{d{x_i}} \\bigg|_{\\vec{x}=\\alpha} = \\epsilon \\text{-coefficient of } f_j(\\alpha + 1\\epsilon) \\ \\ j = 1, \\ldots, m$$\n",
    "\n",
    "Note that this technique requires us to compute a gradient for each of the input variables $x_i$, which means our computational cost scales as $O(n)$ where $n$ is the number of input variables.  Hence, forward mode is preferred in cases with more output variables than input variables.  More generally, however, we motivated a software implementation of the dual numbers, which we go into greater depth below.\n",
    "\n",
    "After reviewing the literature ([1](https://arxiv.org/pdf/1811.05031.pdf), [2](http://www.jmlr.org/papers/volume18/17-468/17-468.pdf), and [3](https://www.mcs.anl.gov/papers/P1152.pdf)), we concluded that we'd make use of _operator overloading_ for implementing dual numbers for the forward mode of automatic differentiation.\n",
    "\n",
    "Our classes will be called \\_DualNumber(), AD\\_fun(DualNumber), Parallelized(AD\\_fun), and PostProcess().  We break down the functionality, method names, core data strctures of each class separately below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DualNumber()\n",
    "DualNumber() will enable us to get the dual representation of a scalar. The user may interact with the class to set up scalar- or vector-valued variables that will later be inputted into functions.  For example, the user may specify $x = AD.DualNumber()$ and later use this $x$ as a variable in a function.\n",
    "\n",
    "Pseudocode for this class is included below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "class DualNumber():\n",
    "    '''\n",
    "    Description: a class to hold dual number representations of vectors/scalars.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, real_part, imag_part=1):\n",
    "        if type(real_part) or type(imag_part) == str:\n",
    "            # Ensure that the input into constructor is valid\n",
    "            raise(ValueError('The input cannot be string'))\n",
    "            \n",
    "        elif len(real_part) != len(image_part):\n",
    "            raise ValueError\n",
    "            \n",
    "        else:\n",
    "            # can be scalar or np.array (i.e. component-wise representation of dual number)\n",
    "            self.real=x\n",
    "            self.imag=1 # set initial imaginary part to 1, since this represents the derivative of x\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AutoDiff()\n",
    "\n",
    "This class will form the crux of our automatic differentation implementation.  In particular, here we will make use of _operator overloading_ to return the function values and derivatives.  Note that the user should\n",
    "\n",
    "So, for example, the user should write \n",
    "1. x = AD.AutoDiff(x)\n",
    "2. function = AD.AutoDiff.sin(x)\n",
    "3. function.get_value(5)\n",
    "\n",
    "to get the value of $\\sin(x)$ at $x=5$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "import numpy as np\n",
    "class AutoDiff():\n",
    "    #Automatic differentiation function class. inherented from Dual number class\n",
    "    def __init__(self, x, dim):\n",
    "        self._value=(lambda self, x: array of size dim, initialized with xs)\n",
    "        self._gradient=(lambda self, x: np.ones(dim)) #Define the function gradient, intended use is private, note that this is a function\n",
    "\n",
    "    def exp(self, x):\n",
    "        self._value= (lambda self, x: np.exp(x._value()))\n",
    "        self._gradient = (lambda self, x: np.exp(x._value())*x._gradient())\n",
    "        \n",
    "    def __add__(self, second_var):\n",
    "        #Override default adding with dunder method\n",
    "        self._value = (lambda self,second_var: self._value() + second_var._value())\n",
    "        self._gradient = (lambda self,second_var: self._gradient() + second_var._gradient())\n",
    "        \n",
    "    def __radd__(self,second_var):\n",
    "        # This function should be able to handle left and right add\n",
    "        \n",
    "    def __mul__(self,second_var):\n",
    "        #Similarly, we can override mutiply\n",
    "    def __rmul__(self,second_var):\n",
    "        #Similarly, we can override right mutiply\n",
    "        \n",
    "    def __sub__(self,second_var):\n",
    "        #Similarly, we can override subtraction\n",
    "    def __rsub__(self,second_var):\n",
    "        #Similarly, we can override right subtraction\n",
    "        \n",
    "    def __div__(self, second_var):\n",
    "    def __rdiv__(self, second_var):\n",
    "        \n",
    "    def sin(self,x):\n",
    "        #Similarly, we can give user defined sin function, \n",
    "        self._value=(lambda self, x: np.sin(x._value()))\n",
    "        self._grad= (lambda self, x: np.cos(x._value())*x._gradient())\n",
    "        \n",
    "    def cos(self,x):#Similarly, we can give user defined cos function, \n",
    "    def log(self,x):#Similarly, we can give user defined log function, \n",
    "        \n",
    "    def get_value(self, value)\n",
    "        return self._value(value)\n",
    "    \n",
    "    def get_gradient(self, value, direc)\n",
    "        convert direc to direc_unit_vector\n",
    "        return np.dot(self._gradient(value), direc_unit_vector)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Note that this function"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
