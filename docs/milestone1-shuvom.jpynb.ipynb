{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview and Notes\n",
    "The purpose of this section is to give a sketch of the mathematical background of automatic differentiation to motivate our implementation of the forward mode."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Software Organization\n",
    "\n",
    "We intend to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation\n",
    "\n",
    "## Classes and Data Structures\n",
    "Recall that automatic differentation is a method for computing the Jacobian of a function $f : \\mathbb{R}^n \\rightarrow \\mathbb{R}^m$ by accumulating and combining partial derivates of elementary functions.  In the _forward mode_ of automatic differentiation, we observe that $g(\\alpha + \\beta \\epsilon) = g(\\alpha) + g'(\\alpha)\\beta\\epsilon$ where $\\alpha + \\beta\\epsilon$ is the dual representation of the real $\\alpha$ (with nilpotent $\\epsilon$, $\\epsilon^2 > 0$).  This observation, along with the chain rule, allows us to compute the Jacobian of $f$ as:\n",
    "$$\\frac{df_j(x)}{d{x_i}} \\bigg|_{\\vec{x}=\\alpha} = \\epsilon \\text{-coefficient of } f_j(\\alpha + 1\\epsilon) \\ \\ j = 1, \\ldots, m$$\n",
    "\n",
    "Note that this technique requires us to compute a gradient for each of the input variables $x_i$, which means our computational cost scales as $O(n)$ where $n$ is the number of input variables.  Hence, forward mode is preferred in cases with more output variables than input variables.  More generally, however, we motivated a software implementation of the dual numbers, which we go into greater depth below.\n",
    "\n",
    "After reviewing the literature ([1](https://arxiv.org/pdf/1811.05031.pdf), [2](http://www.jmlr.org/papers/volume18/17-468/17-468.pdf), and [3](https://www.mcs.anl.gov/papers/P1152.pdf)), we concluded that we'd make use of _operator overloading_ for implementing dual numbers for the forward mode of automatic differentiation.\n",
    "\n",
    "Our classes will be called \\_DualNumber(), AD\\_fun(DualNumber), Parallelized(AD\\_fun), and PostProcess().  We break down the functionality, method names, core data strctures of each class separately below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \\_DualNumber()\n",
    "\\_DualNumber() will enable us to calculate the dual representation of scalar or a vector. A vectorized dual number would just return the dual representation of each component. Pseudocode for this class is included below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Cell magic `%%cache` not found.\n"
     ]
    }
   ],
   "source": [
    "# cache makes sure this cell won't fully run (i.e. stay as pseudocode, but with all the coloring)\n",
    "%%cache\n",
    "\n",
    "class DualNumber():\n",
    "    '''\n",
    "    Description: a class to hold dual number representations of vectors/scalars.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, real_part=None,imag_part=None):\n",
    "        if type(real_part) or type(imag_part) == str:\n",
    "            # Ensure that the input into constructor is valid\n",
    "            raise(ValueError('The input cannot be string'))\n",
    "            \n",
    "        elif real_part or imag_part is vector:\n",
    "        # If input type is list/tuple/np.array, data type conversion handled by another function\n",
    "            self.convert_data_type()\n",
    "        else:\n",
    "            self.real=real_part\n",
    "            self.imag=imag_part\n",
    "            \n",
    "    def convert_data_type():\n",
    "        real = []\n",
    "        imag = []\n",
    "        for component in vector:\n",
    "            real.append()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AD\\_fun()\n",
    "\n",
    "This class will form the crux of our automatic differentation implementation.  In particular, here we will make use of _operator overloading_ to return the "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
