{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Describe the problem the software solves and why it's important to solve that problem.\n",
    "\n",
    "Differentiation has ubiquitous applications in many areas of mathematics, sciences and engineering. As such, it is certainly useful and convenient if computer programs could carry out differentiation automatically for application in a wide variety of cases. For computationally heavy projects, the ability to compute derivatives automatically becomes even more critical as manually working out deriatives in such projects is certainly an impossible task. Even though there exists methods such as *numerical diffentiation* and *symbolic differentiation* in determining derivatives computationally, these two methods have their limitations. In the following, we shall briefly review *numerical diffentiation* and *symbolic differentiation* to highlight some of their difficulties before moving on to describing *automatic differentiation* and the advantages it brings over the other two methods.   \n",
    "\n",
    "### Numerical Differentiation\n",
    "In *numerical differentiation*, the value of derivatives is approximated using the following formula:\n",
    "\n",
    "$$\n",
    "\\frac{\\partial{f(x)}}{\\partial{x}} \\approx \\frac{f(x+h)-f(x)}{h}\n",
    "$$\n",
    "\n",
    "However, when the h values are too small, the numerical approximation fluctuates about the analytical answer. This is because the step size is too small, leading to a round-off error of the floating points caused by the limited precision of computations. On the other hand, when the h values are too large, the numerical approximation becomes inaccurate. This is because the step size is too big, leading to an error of approximation known as truncation error.\n",
    "\n",
    "### Symbolic Differentiation\n",
    "In *symbolic differentiation*, expressions are manipulated automatically to obtain the required derivatives. At its heart, *symbolic differentiation* applies transformations that captures the various rules of differentiation in order to manipulate the expressions. However, *symbolic differentiation* requires careful and sometimes ingenious design as accidental manipulation can easily produce large expressions which take up a lot of computational power and time, which leads to a problem known as expression swell.\n",
    "\n",
    "### Automatic Differentiation\n",
    "As seen from above, both *numerical diffentiation* and *symbolic differentiation* have their respective issues when it comes to computing derivatives. These issues are further exacerbated when calculating higher order derivatives, where both errors and complexity increases. *Automatic differentiation* overcomes these issues by recognizing that every differentiation, no matter how complicated, can be executed in a stepwise fashion with each step being an execution of either the elementary arithmetic operations (addition, substraction, multiplication, division) or the elementary functions (sin, sqrt, exp, log, etc.). To track the evaluation of each step, *automatic differentiation* produces computational graphs and evaluation traces. To compute the derivatives, *automatic differentiation* applies the chain rule repeatedly at all steps. By taking a stepwise approach and using the chain rule, *automatic differentiation* circumvents the issues encountered by both *numerical diffentiation* and *symbolic differentiation* and automatically compute derivatives that are both accurate and with a high level of precision. In order to further understand *automatic differentiation*, we present the mathematical background and essential ideas of *automatic differentiation* in the next section.\n",
    "\n",
    "Note - In our research of automatic differentiation, we referred to the following resources:\n",
    "\n",
    "Baydin, A.G., Pearlmutter, B.A., Radul, A. A. & Siskind, J.M. (2018). Automatic differentiation in machine learning: A survey. *Journal of Machine Learning Research, 18*, 1-42.\n",
    "\n",
    "Geeraert, S., Lehalle, C.A., Pearlmutter, B., Pironneau, O. & Reghai, A. (2017). Mini-symposium on automatic differentiation and its applications in the financial industry. *ESAIM: Proceedings and Surverys* (pp. 1-10).\n",
    "\n",
    "Berland, H. (2006). *Automatic differentiation* [PowerPoint Slides]. Retrieved from http://www.robots.ox.ac.uk/~tvg/publications/talks/autodiff.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How to Use Package\n",
    "\n",
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have used PyPI to host our package. Users can download our Automatic Differentiation package with the following command:\n",
    "\n",
    "`pip install autodiffing`\n",
    "\n",
    "To install the required dependencies, users need to run the following command:\n",
    "\n",
    "`pip install -r requirements.txt`\n",
    "\n",
    "Within our requirements.txt, we have the following packages:\n",
    "\n",
    "`bleach==3.1.0`\\\n",
    "`certifi==2019.9.11`\\\n",
    "`cycler==0.10.0`\\\n",
    "`docutils==0.15.2`\\\n",
    "`kiwisolver==1.1.0`\\\n",
    "`matplotlib==3.1.1`\\\n",
    "`nltk==3.4.5`\\\n",
    "`numpy==1.17.4`\\\n",
    "`Pygments==2.4.2`\\\n",
    "`pyparsing==2.4.5`\\\n",
    "`python-dateutil==2.8.1`\\\n",
    "`readme-renderer==24.0`\\\n",
    "`requests-toolbelt==0.9.1`\\\n",
    "`scipy==1.3.2`\\\n",
    "`six==1.13.0`\\\n",
    "`twine==2.0.0`\\\n",
    "`webencodings==0.5.1`\\\n",
    "\n",
    "Most of the packages above come with the installation of `python` version 3.7. \n",
    "\n",
    "Our team has selected only 3 additional packages for our user to install, namely `matplotlib`, `scipy` and `numpy`. \n",
    "\n",
    "`numpy` is essential for our Automatic Differentiation package as we require it for the calculation of our elementary functions, and for dealing with arrays and matrices when there are vector functions and vector inputs.\n",
    "\n",
    "`scipy` is a good package to have for its optimization and linear algebra abilities. In particular, under our future features for the Automatic Differentiation package, we hope to be able to deal with optimization problems. \n",
    "\n",
    "`matplotlib` is needed for any potential visualization of our outputs. `bleach`, `docutils`, `Pygments` are additional packages that `matplotlib` requires.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the Package\n",
    "\n",
    "Once the users have installed all the dependencies and the package itself, they may begin to use our package to quickly find derivatives of functions.  For this section, we walk through three different examples of how users can interact with the package for their purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Future Features\n",
    "\n",
    "Future features for our Automatic Differentiation package include taking in vector inputs and vector functions and implementing reverse mode for automatic differentiation. For each of these future features, their required software changes and primary challenges are elaborated below.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Inputs \n",
    "\n",
    "To deal with vector inputs and vector functions, we make use of our current package that deals with only scalar function of scalar input. Specifically speaking, we first create scalar functions that can deal with vector inputs before considering vector functions. A new class `scalar_func` is created that inherits from the `DualNumber` class. In this class, we define new methods that have their equivalent in the `DualNumber` class and determine values of the scalar function and its derivatives by looping over the array of vector inputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class scalar_func(DualNumber):\n",
    "    # To deal with scalar functions with vector inputs\n",
    "    def __init__(self, vector_inputs,seed_vector):\n",
    "    # check dimension\n",
    "    assert len(seed_vector) == len(vector_inputs)\n",
    "    self._inputs = vector_inputs\n",
    "    self._val= np.empty()\n",
    "    self._der= seed_vector\n",
    "    \n",
    "    def val(self):\n",
    "        for i in self._inputs:\n",
    "            self._val = np.append([self._val],[i.val()],axis=0)\n",
    "        return self._val\n",
    "    \n",
    "    def Jacobian(self):\n",
    "        # Calculate Jacobian vector given the vector_inputs\n",
    "        for i in self._inputs:\n",
    "            jacobian = np.append([jacobian],[i.der()],axis=0)\n",
    "        return jacobian\n",
    "    \n",
    "    def der(self):\n",
    "        # determine result of derivatives\n",
    "        self._der = np.dot(self.Jacobian(), self._der)\n",
    "        return self._der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector Functions\n",
    "\n",
    "Vector functions build on the work that is done for scalar and vector inputs. A new class `vector_func` is created that inherits from the `scalar_func` class. In this class, we define new methods that have their equivalent in the `scalar_func` class and the general approach is to loop over the list of scalar functions while applying the equivalent methods in the `scalar_func` class to determine the outputs for the vector functions. For instance, in the `Jacobian` method, we calculate the Jacobian matrix by simply looping over the list of `scalar functions` and applying the `Jacobian` method on each scalar function. The result for the derivatives is determined using the dot product betwen the Jacobian matrix and seed vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class vector_func(scalar_func):\n",
    "    # To deal with vector functions with scalar or vector inputs, inherits from the scalar_func class\n",
    "    \n",
    "    def __init__(self, scalar_functions,seed_vector):\n",
    "    # check dimension\n",
    "    assert len(seed_vector) == len(scalar_functions)\n",
    "    self._functions = scalar_functions\n",
    "    self._val=np.empty()\n",
    "    self._der= seed_vector\n",
    "    \n",
    "    def val(self):\n",
    "        for func in self._functions:\n",
    "            self._val = np.append([self._val],[func.val()],axis=0)\n",
    "        return self._val\n",
    "    \n",
    "    def Jacobian(self):\n",
    "        # Calculate Jacobian matrix given the scalar_functions\n",
    "        for func in self._functions:\n",
    "            jacobian_matrix = np.append([jacobian_matrix],[func.Jacobian()],axis=0)\n",
    "        return jacobian_matrix\n",
    "    \n",
    "    def der(self):\n",
    "        # determine result of derivatives\n",
    "        self._der = np.dot(self.Jacobian(), self._der)\n",
    "        return self._der"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary challenge for the implementation of both vector inputs and vector functions is the design of the code such that users can interact with our package in the most straightforward and easily understood manner. For example, the team is considering collapsing both `scalar_func` and `vector_func` into a single `func` class so that the user only have to call upon one class when defining functions. In addition, we have to think of a way to tackle the case when users decide to create a function using `DualNumber` directly (ie not using our classes for functions). In that case, perhaps we should make `DualNumber` a private class and let users use `func` directly to create variables (ie treat variables as a scalar function of scalar input). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reverse Mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The reverse mode is fundamentally different in its approach to automatic differentiation as compared to the forward mode. In particular, the reverse mode consists of both the forward pass and reverse pass, with no chain rule applied in the forward pass (only partial derivatives are stored). The result of a reverse mode is only determined after the reverse pass is done, and the value of each variable or parent node at each stage depends on the values of its children nodes. As such, this has three important implications for the design of our package for reverse mode. \n",
    "\n",
    "Firstly, the reverse mode cannot be interpreted in the context of dual numbers like the forward mode and we need to come up with a different class for the implementation of reverse mode. Since the result cannot be calculated until the reverse pass is done and the variables at each stage of the reverse mode depends on the values of its children, we need to instantiate a reverse mode object/variable with an empty list that will temporarily hold the partial derivative values of its children during the forward pass. Note that we need a list here because it is possible for a parent node to have more than one child node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseVar():\n",
    "    def __init__(self, value):\n",
    "        self._value = value # value of variable at which the derivative is determined\n",
    "        self._children = [] # empty list to contain partial derivatives of children during forward pass\n",
    "        self._der = None # value not determined until reverse pass is done\n",
    "        \n",
    "    def val(self):\n",
    "        return self._value\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Secondly, as the forward pass only does partial derivatives and does not apply the chain rule, we need to redefine the overloading of operators for our reverse mode objects/variables. As an example, the overloading of the multiplication operator is shown below. Note that overloading the operators in essence is equivalent to carrying out the forward pass, and the partial derivatives are stored as temporary items within `self._children` for evaluation later during reverse pass."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseVar():\n",
    "    def __init__(self, value):\n",
    "        self._value = value # value of variable at which the derivative is determined\n",
    "        self._children = [] # empty list to contain partial derivatives of children during forward pass\n",
    "        self._der = None # value not determined until reverse pass is done\n",
    "        \n",
    "    def val(self):\n",
    "        return self._value    \n",
    "        \n",
    "    def __mul__(self, other):\n",
    "        z = ReverseVar(self._value * other._value)\n",
    "        self._children.append((other._value, z)) \n",
    "        other._children.append((self._value, z)) \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we define a method `der` to carry out the reverse pass recursively in order to calculate the value of the derivatives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReverseVar():\n",
    "    def __init__(self, value):\n",
    "        self._value = value # value of variable at which the derivative is determined\n",
    "        self._children = [] # empty list to contain partial derivatives of children during forward pass\n",
    "        self._der = None # value not determined until reverse pass is done\n",
    "    \n",
    "    def val(self):\n",
    "        return self._value\n",
    "        \n",
    "    def der(self):\n",
    "        # recurse only if the derivative is not yet calculated\n",
    "        if self._der is None:\n",
    "            # calculate derivative using chain rule\n",
    "            for weight, var in self._children:\n",
    "                self._der = sum(weight * var.der())\n",
    "        return self._der\n",
    "\n",
    "    def __mul__(self, other):\n",
    "        z = Var(self._value * other._value)\n",
    "        self._children.append((other._value, z)) \n",
    "        other._children.append((self._value, z)) \n",
    "        return z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary challenge for reverse mode is to ensure that its classes, methods and attributes are kept separate from that of forward mode even though we would want them to share certain similarity. For example, we would want the user to use the same methods in each mode to get out the same results. In addition, after writing code for the reverse mode to deal with scalar functions of scalars, we ought to extend the reverse mode to deal with vector inputs and vector functions just like the forward mode did."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization\n",
    "\n",
    "When users interact with our package, it would be useful for them to have a way of visualizing the on-going calculations or final results. We define a new method called `post_process` which will be found in the different classes of our package where visual outputs of either key calculations or important results is possible. For instance, `post_process` within the reverse mode class might produce tables of forward pass and reverse pass. The method `post_process` primarily uses the `matplotlib` library and takes in `directory_out` as an argument for users to indicate the directory in which they wish to save the visualization outputs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process(directory_out):\n",
    "    #Visualization of relevant calculations and results goes here\n",
    "    fig, ax = plt.figure()\n",
    "    plt.savefig('{}/figure_title.pdf'.format(directory_out))\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
